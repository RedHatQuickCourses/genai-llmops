<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Module 1: Establishing a Performance Baseline :: LLM Operations Optimization and Inference</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Ops: Your Adventure Begins</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Model Performance Benchmarking with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Optimizing vLLM Performance</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Course: Optimizing vLLM Performance</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="section1.html">Module 1: Establishing a Performance Baseline</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="section2.html">Module 2: Hands-On Parameter Tuning</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="section3.html">Module 3: Advanced Optimization with Quantization</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="section4.html">Course Wrap-up</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="section5.html">LLM Ops: Your Adventure Begins</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/intro.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section1.html">Module 1: Your First Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section2.html">Check your work</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section3.html">Module 2: Automating Quantization with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section4.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="intro.html">Optimizing vLLM Performance</a></li>
    <li><a href="index.html">Course: Optimizing vLLM Performance</a></li>
    <li><a href="section1.html">Module 1: Establishing a Performance Baseline</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Module 1: Establishing a Performance Baseline</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Before we can optimize, we must measure. In this module, we will deploy a model with a set of initial parameters and use the GuideLLM pipeline to establish a performance baseline. This gives us a starting point to compare our future optimizations against.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_1_1_deploying_the_initial_model_configuration"><a class="anchor" href="#_1_1_deploying_the_initial_model_configuration"></a>1.1 Deploying the Initial Model Configuration</h3>
<div class="paragraph">
<p>We&#8217;ll start by deploying the <code>granite-8b</code> model with specific arguments for a chat application scenario: handling up to 32 concurrent users with a maximum sequence length of 2048 tokens.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Ensure you are in your <code>vllm</code> project in the OpenShift CLI.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">oc project vllm</code></pre>
</div>
</div>
</li>
<li>
<p>Review the initial deployment configuration in the <code>workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml</code> file. We are setting <code>--max-num-seqs=32</code> and <code>--max-model-len=2048</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=2048"</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the model using Helm. This command will uninstall any previous <code>granite-8b</code> deployment and install the new one.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">helm uninstall granite-8b &amp;&amp; \
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.11 -f workshop_code/deploy_vllm/vllm_rhoai_custom_2/values.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the model to fully deploy and show a green checkmark in the OpenShift AI dashboard.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_1_2_setting_up_the_benchmark_pipeline"><a class="anchor" href="#_1_2_setting_up_the_benchmark_pipeline"></a>1.2 Setting Up the Benchmark Pipeline</h3>
<div class="paragraph">
<p>We will use the same GuideLLM Tekton pipeline from the previous course to run our benchmarks.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you already deployed these resources in the GuideLLM course within the <code>vllm</code> project, you can skip this step.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the pipeline repository and apply the necessary resources.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">git clone https://github.com/jhurlocker/guidellm-pipeline.git
oc apply -f guidellm-pipeline/pipeline/upload-results-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-pipeline.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/pvc.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n vllm
oc apply -f guidellm-pipeline/pipeline/mino-bucket.yaml -n vllm</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_1_3_running_the_baseline_benchmark"><a class="anchor" href="#_1_3_running_the_baseline_benchmark"></a>1.3 Running the Baseline Benchmark</h3>
<div class="paragraph">
<p>Now, let&#8217;s run the pipeline to test our initial configuration under increasing concurrent loads (1, 4, 8, and 16 users).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Update the <code>target</code> parameter in the <code>guidellm-pipeline/pipeline/guidellm-pipelinerun.yaml</code> file with your model&#8217;s inference endpoint URL.</p>
</li>
<li>
<p>Create the pipeline run.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">oc create -f guidellm-pipeline/pipeline/guidellm-pipelinerun.yaml -n vllm</code></pre>
</div>
</div>
</li>
<li>
<p>After the pipeline finishes, use the <code>graph_benchmarks.ipynb</code> notebook (provided in the <code>guidellm_notebook_charts</code> directory) to download the results from MinIO and visualize the median TTFT.</p>
<div class="imageblock unresolved">
<div class="content">
<img src="ttft_chart.png" alt="TTFT chart">
</div>
</div>
</li>
<li>
<p>Notice how quickly the latency increases as we add concurrent users. This is our baseline. Rename the downloaded results file to <code>benchmark_1.txt</code> for later comparison.</p>
</li>
</ol>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Course: Optimizing vLLM Performance</a></span>
  <span class="next"><a href="section2.html">Module 2: Hands-On Parameter Tuning</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
