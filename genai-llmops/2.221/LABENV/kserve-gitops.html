<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GitOps with KServe :: LLM Operations Optimization and Inference</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#intro.adoc">intro.adoc</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section1.html">Module 1: Your First Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section2.html">Check your work</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section3.html">Module 2: Automating Quantization with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section4.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="kserve-gitops.html">GitOps with KServe</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">GitOps with KServe</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Up to this point, we have been primarily using the OpenShift AI Dashboard to deploy our models.  While this is a great way to get started, it does not allow us to utilize the full capabilities of KServe, or manage our models using standard GitOps practices that may be required for a production environment.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
For a deeper dive on more GitOps capabilities with OpenShift AI, see the <a href="https://ai-on-openshift.io/odh-rhoai/gitops/#model-serving">AI on OpenShift GitOps</a> page.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kserve_objects"><a class="anchor" href="#_kserve_objects"></a>KServe Objects</h2>
<div class="sectionbody">
<div class="paragraph">
<p>KServe uses two primary objects to manage model serving:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>LLMInferenceService (previously InferenceService)</p>
</li>
<li>
<p>ServingRuntime</p>
</li>
</ul>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>Think of KServe&#8217;s <strong>InferenceService</strong> as a versatile Swiss Army knife; it&#8217;s the general-purpose, flexible tool designed to deploy any kind of machine learning model, from traditional classifiers to standard deep learning networks.</p>
</div>
<div class="paragraph">
<p>In contrast, the <strong>LLMInferenceService</strong> is a specialized scalpel, crafted exclusively for the unique and demanding requirements of serving Large Language Models (LLMs). It simplifies the entire process by providing a more straightforward configuration and automatically including critical optimizations—such as advanced token-based autoscaling and native support for high-performance runtimes like vLLM—that you would otherwise have to build and manage yourself.</p>
</div>
<div class="paragraph">
<p>In short, use InferenceService for broad ML model serving, but choose the streamlined and powerful LLMInferenceService when deploying large language models.</p>
</div>
</div>
</div>
<div class="paragraph">
<p>The LLMInferenceService object is used to define the model and any model specific configurations to serve that model, and the ServingRuntime object is used to define the runtime environment for the model.</p>
</div>
<div class="paragraph">
<p>OpenShift AI generally assumes a one to one relationship between an LLMInferenceService and a ServingRuntime, but a ServingRuntime can be configured to be used by multiple LLMInferenceServices.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_vllm_kserve_helm_chart"><a class="anchor" href="#_vllm_kserve_helm_chart"></a>vLLM KServe Helm Chart</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To help manage the deployment of vLLM, a Helm chart is available from Red Hat Services that can be used to help deploy a vLLM instance using KServe:</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve" class="bare">https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve</a></p>
</div>
<div class="paragraph">
<p>The chart can be used to deploy an instance using vLLM and exposes many features that are not available when using the OpenShift AI Dashboard.</p>
</div>
<div class="sect2">
<h3 id="_using_the_helm_chart_with_gitops"><a class="anchor" href="#_using_the_helm_chart_with_gitops"></a>Using the Helm Chart with GitOps</h3>
<div class="paragraph">
<p>While the Helm chart itself is not true GitOps, it does get us one step closer to being able to manage our models using GitOps practices.  With the Helm chart we can easily use ArgoCD&#8217;s native Helm support to deploy the chart with a values.yaml file, or we can use something like Kustomize to help inflate the chart:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

helmCharts:
- name: vllm-kserve
  releaseName: my-llm-server
  version: 0.5.9
  repo: https://redhat-ai-services.github.io/helm-charts/
  valuesFile: values.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>With this configuration we can point an ArgoCD application directly at the Kustomize configuration and have it deploy the chart with the values.yaml file.</p>
</div>
<div class="paragraph">
<p>For more options for how to deploy Helm Charts with ArgoCD, take a look at the blog post: <a href="https://developers.redhat.com/articles/2023/05/25/3-patterns-deploying-helm-charts-argocd" class="bare">https://developers.redhat.com/articles/2023/05/25/3-patterns-deploying-helm-charts-argocd</a></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_deploy_additional_models_with_helm"><a class="anchor" href="#_lab_deploy_additional_models_with_helm"></a>Lab: Deploy Additional Models with Helm</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this lab, we will deploy the final LLM we need for the later Llama-stack labs.  This time, we will use the Helm chart from above to deploy the model.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you don&#8217;t already have Helm installed, please refer to the official documentation: <a href="https://helm.sh/docs/intro/install/" class="bare">https://helm.sh/docs/intro/install/</a>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>To begin, we will add the Red Hat AI Services Helm repository to our Helm client and pull the latest charts:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services</code></pre>
</div>
</div>
</li>
<li>
<p>Next, we will update our Helm client to pull the latest charts from the repository:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm upgrade -i granite-guardian -n vllm redhat-ai-services/vllm-kserve --version 0.5.111 \
  --set fullnameOverride="granite-guardian" \
  --set model.uri="oci://quay.io/redhat-ai-services/modelcar-catalog:granite-guardian-3.2-5b" \
  --set model.args={"--max-model-len=20000"} \
  --set deploymentMode=RawDeployment \
  --set scaling.rawDeployment.deploymentStrategy.type=Recreate</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>fullnameOverride</code> is used to override the default name of the deployment.  Helm charts usually default to using a combination of the release name and the chart name to create the name of the deployment.  In this case, we are using the release name <code>granite-guardian</code> and ignoring we want to skip including the chart name in the deployment.</p>
</div>
<div class="paragraph">
<p><code>image.tag</code> is used to define the tag of the vLLM image to use.  This is an optional setting, but can help to ensure that we are using the same version of vLLM that is being used when we deploy using the OpenShift AI Dashboard.</p>
</div>
<div class="paragraph">
<p><code>model.uri</code> is used to define the model to be served from an existing OCI container.</p>
</div>
<div class="paragraph">
<p><code>model.args</code> is used to define any model specific arguments to be passed to the model.  In this case, we are setting the <code>--max-model-len</code> to 20000 to limit the context length of the model in order to allow it to fit our our GPU.</p>
</div>
<div class="paragraph">
<p><code>deploymentMode</code> is used to define the deployment mode we want KServe to use.  Setting this option to <code>RawDeployment</code> allows us to deploy the model without the ServiceMesh/Serverless.</p>
</div>
<div class="paragraph">
<p><code>scaling.rawDeployment.deploymentStrategy.type</code> will allow us to set the deployment strategy to <code>Recreate</code> which will allow us to make updates to the model in the future by first deleting the existing deployment and then creating a new one.  This option is generally not recommended for production environments as it can cause downtime for the model, but it is useful for us since we have a limited number of GPUs and we don&#8217;t need to worry about downtime.</p>
</div>
</li>
<li>
<p>To verify that the deployment was successful, check that the pod successfully started and is in the ready state.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_optional_lab_redeploy_previous_models_with_gitops"><a class="anchor" href="#_optional_lab_redeploy_previous_models_with_gitops"></a>Optional Lab: Redeploy previous models with GitOps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In our previous lab sections, we deployed our models using the OpenShift AI Dashboard.  While this is a great way to get started, it doesn&#8217;t allow us to easily manage our models using GitOps practices.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Helm will not allow us to redeploy the same model with the same name, so we will need to delete the existing InferenceService and ServingRuntime.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete inferenceservice granite-8b -n vllm
oc delete servingruntime granite-8b -n vllm</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you want to redeploy the objects with Helm without downtime or deleting the existing objects, you can manually add the necessary labels/annotations to the objects that helm complains about when running the <code>helm upgrade</code> or <code>helm install</code> commands.</p>
</div>
<div class="paragraph">
<p>Run the <code>helm upgrade</code> command to see the errors and what labels/annotations are required to be added to the objects.</p>
</div>
<div class="paragraph">
<p>If you are updating the objects using ArgoCD, these manually changes should not be necessary.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Next, we can deploy the model with the same options we used in the previous lab.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">helm upgrade -i granite-8b -n vllm redhat-ai-services/vllm-kserve --version 0.5.11 \
  --set fullnameOverride="granite-8b" \
  --set model.uri="oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct" \
  --set 'resources.requests.cpu=4' \
  --set 'resources.limits.cpu=8' \
  --set 'resources.requests.memory=8Gi' \
  --set 'resources.limits.memory=16Gi' \
  --set 'resources.requests.nvidia\.com/gpu=2' \
  --set 'resources.limits.nvidia\.com/gpu=2'  \
  --set model.args={"--tensor-parallel-size=2"}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the pod logs to ensure that the pod successfully starts.</p>
</div>
</li>
<li>
<p>The same helm chart also supports Mutli-node deployments, however, it is recommended to only use the multi-node deployment configuration with OpenShift AI 2.22 or later.</p>
<div class="paragraph">
<p>Use the helm chart documentation to help construct a <code>helm upgrade</code> command that would allow us to create the same configuration with the helm chart that we deployed in the Multi-node vLLM lab.  You can find documentation for all of the available options on the GitHub repo here:</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve" class="bare">https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve</a></p>
</div>
<div class="paragraph">
<p>Just like before, you will need to delete the existing InferenceService and ServingRuntime before running the <code>helm upgrade</code> command.</p>
</div>
<div class="paragraph">
<p>Refer to the answer below to compare your crafted command with the correct one.</p>
</div>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_multi_node_vllm_solution"><a class="anchor" href="#_multi_node_vllm_solution"></a>Multi-node vLLM Solution</h3>
<details>
<summary class="title">Details</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">helm upgrade -i vllm-multi-node-llama -n vllm redhat-ai-services/vllm-kserve --version 0.5.11 \
  --set fullnameOverride="vllm-multi-node-llama" \
  --set model.uri="pvc://llama-model/Llama-3.3-70B-Instruct-quantized.w4a16" \
  --set servingTopology=multiNode \
  --set deploymentMode=RawDeployment \
  --set multiNode.pipelineParallelSize=2 \
  --set multiNode.tensorParallelSize=2 \
  --set 'resources.requests.cpu=4' \
  --set 'resources.limits.cpu=8' \
  --set 'resources.requests.memory=8Gi' \
  --set 'resources.limits.memory=16Gi' \
  --set 'resources.requests.nvidia\.com/gpu=2' \
  --set 'resources.limits.nvidia\.com/gpu=2'</code></pre>
</div>
</div>
</div>
</details>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
