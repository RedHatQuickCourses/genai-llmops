<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Advanced vLLM Configuration :: LLM Operations Optimization and Inference</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#intro.adoc">intro.adoc</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section1.html">Module 1: Your First Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section2.html">Check your work</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section3.html">Module 2: Automating Quantization with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section4.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="vllm-configuration.html">Advanced vLLM Configuration</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Advanced vLLM Configuration</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>vLLM provides a number of options that can be set for the vLLM server using the arguments on the OpenShift AI Dashboard or the InferenceService object.</p>
</div>
<div class="paragraph">
<p>To explore the available options, you can run <code>vllm --help</code> or refer to the docs here:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.vllm.ai/en/latest/cli/index.html" class="bare">https://docs.vllm.ai/en/latest/cli/index.html</a></p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
vLLM is a rapidly evolving project and customers may not be running the latest version of vLLM.  When referencing the upstream docs it is recommended that you look at the docs for the specific version of vLLM that you are running as the options may have changed.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setting_arguments"><a class="anchor" href="#_setting_arguments"></a>Setting Arguments</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Arguments can be set directly through the UI for single-node vLLM instances.  Any configuration added to the UI is represented in the InferenceService object in the <code>args</code> field:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: my-model-server
spec:
  predictor:
    model:
      args:
        - '--max-model-len=10000'</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_common_config_options"><a class="anchor" href="#_common_config_options"></a>Common Config Options</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_model_configs"><a class="anchor" href="#_model_configs"></a>Model Configs</h3>
<div class="sect3">
<h4 id="_served_model_name"><a class="anchor" href="#_served_model_name"></a>--served-model-name</h4>
<div class="paragraph">
<p><code>--served-model-name</code> is set by default in the ServingRuntime to be equal to the name of the ServingRuntime object.  If you wish to override this name, you can set the argument manually and specify the model name you wish to utilize.</p>
</div>
</div>
<div class="sect3">
<h4 id="_optimization_options"><a class="anchor" href="#_optimization_options"></a>Optimization Options</h4>

</div>
<div class="sect3">
<h4 id="_max_model_len"><a class="anchor" href="#_max_model_len"></a>--max-model-len</h4>
<div class="paragraph">
<p><code>--max-model-len</code> is commonly used to help define the size of the KV Cache.  When attempting to run a model on a GPU that does not have enough vRAM to support the models default max context length you will need to set this value to limit the size of the KV Cache.</p>
</div>
</div>
<div class="sect3">
<h4 id="_gpu_memory_utilization"><a class="anchor" href="#_gpu_memory_utilization"></a>--gpu-memory-utilization</h4>
<div class="paragraph">
<p><code>--gpu-memory-utilization</code> is set to 90% by default.  You can increase this value to give the vLLM instance access to more vRAM.  vLLM generally does need a little bit of vRAM left free for some processes, but you can generally safely increase this value a bit to expand the size of the KV Cache, especially when using larger GPUs such as an H100 with 80Gb of vRAM.</p>
</div>
</div>
<div class="sect3">
<h4 id="_enable_eager"><a class="anchor" href="#_enable_eager"></a>--enable-eager</h4>
<div class="paragraph">
<p><code>--enable-eager</code> disables the creation of a CUDA graph that is used to improve performance of processing requests.  As a trade off for potentially slower responses, vLLM is able to free up additional vRAM which can be used to increase the size of the KV Cache.</p>
</div>
<div class="paragraph">
<p>This option can be useful for increasing the overall size of the KV Cache when response time is not as important as a larger context length.</p>
</div>
</div>
<div class="sect3">
<h4 id="_max_num_seqs"><a class="anchor" href="#_max_num_seqs"></a>--max-num-seqs</h4>
<div class="paragraph">
<p><code>--max-num-seqs</code> can be used to set limits for how many concurrent requests are processed at any given time.  This option can be set to prevent the model server from creating contention issues and providing better response times for most users.</p>
</div>
</div>
<div class="sect3">
<h4 id="_limit_mm_per_prompt"><a class="anchor" href="#_limit_mm_per_prompt"></a>--limit-mm-per-prompt</h4>
<div class="paragraph">
<p><code>--limit-mm-per-prompt</code> is an option to limit the number of mutli-modal inputs per request.  This option can help to limit requests from sending too many large mutli-modal items which require a large number of resources to compute.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parallelism"><a class="anchor" href="#_parallelism"></a>Parallelism</h3>
<div class="sect3">
<h4 id="_tensor_parallel_size"><a class="anchor" href="#_tensor_parallel_size"></a>--tensor-parallel-size</h4>
<div class="paragraph">
<p><code>--tensor-parallel-size</code> generally corresponds to the number of GPUs that are available for vLLM.</p>
</div>
</div>
<div class="sect3">
<h4 id="_pipeline_parallel_size"><a class="anchor" href="#_pipeline_parallel_size"></a>--pipeline-parallel-size</h4>
<div class="paragraph">
<p><code>--pipeline-parallel-size</code> is generally only used with multi-node configurations and corresponds to the number of node.</p>
</div>
</div>
<div class="sect3">
<h4 id="_enable_expert_parallel"><a class="anchor" href="#_enable_expert_parallel"></a>--enable-expert-parallel</h4>
<div class="paragraph">
<p><code>--enable-expert-parallel</code> can be used with MoE models to improve the way that models are sharded across GPUs.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_tool_calling"><a class="anchor" href="#_tool_calling"></a>Tool Calling</h3>
<div class="paragraph">
<p>Tool calling is a common capability that we may need to enable on a vLLM instance using the following common commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">--enable-auto-tool-choice
--tool-call-parser=granite
--chat-template=/app/data/template/tool_chat_template_granite.jinja</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>--enable-auto-tool-choice</code> turns on the ability to do tool calling.  <code>--tool-call-parser</code> is used to define what parser is used when doing tool calling.  In the example, we are using the <code>granite</code> parser, but other parsers exist that can be used with other models.  See the docs for what options are available.</p>
</div>
<div class="paragraph">
<p><code>--chat-template</code> generally corresponds to a template that is aligned with the model.  The templates can be found in the <a href="https://github.com/vllm-project/vllm/tree/main/examples">vLLM GitHub</a> and they should be pre-packaged in the vLLM container image in <code>/app/data/template/</code>.</p>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
