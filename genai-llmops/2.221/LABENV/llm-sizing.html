<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM GPU Requirements :: LLM Operations Optimization and Inference</title>
    <link rel="prev" href="kserve-gitops.html">
    <link rel="next" href="multi-node-multi-gpu.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Advanced vLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kserve-gitops.html">GitOps with KServe</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="llm-sizing.html">LLM GPU Requirements</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="multi-node-multi-gpu.html">Deploying a Model with vLLM on a multiple node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="single-node-multi-gpu.html">Deploying a Model with vLLM on a single node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-configuration.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Module 1: LLM Deployment Strategies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.0-deploy-intro.html">Module 1: LLM Deployment Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.1-deploy-RHEL.html">Deploying RH Inference Server on RHEL</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.2-deploy-ocp.html">Deploying RH Inference Server on OpenShift</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.3-deploy-rhoai.html">Deploy vLLM on OpenShift AI with kserve and model car</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.4-deploy-conclusion.html">Conclusion to Deployment Exercises</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Model Evaluation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/module-2.0-eval-intro.html">Model Evaluation Module</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/module-2.1-eval-performance.html">Evaluating System Performance with GuideLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/module-2.2-eval-accuracy.html">Evaluating Model Accuracy with Trusty AI lm-eval-harness service</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/module-2.3-eval-conclusion.html">Conclusion: From Evaluation to Impact</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/module-3.0-optimization-intro.html">Module 3: vLLM Performance Optimization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#module-3.0-optimization-practice.adoc">module-3.0-optimization-practice.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#module-3.0-optimization-conclusion.adoc">module-3.0-optimization-conclusion.adoc</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Module 4: Model Quantization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.0-quantization-intro.html">Module 4: Model Quantization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.1-quantization.html">Weights and Activation Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.2-quantization.html">Model Quantization Pipeline</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.3-quantization-conclusion.html">Module 4 Conclusion: Model Quantization Mastery</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM Overview</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM and Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">Optimizing with NVIDIA GPU Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">The Red Hat AI Validated Model Repository</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Intro Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">The Granite Model Family</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/guide_llm.html">Performance Benchmarking with GuideLLM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="index.html">Advanced vLLM</a></li>
    <li><a href="llm-sizing.html">LLM GPU Requirements</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">LLM GPU Requirements</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>When selecting hardware to run an LLM, several factors need to be considered including the model size, the KV Cache requirements for the use case, the use case concurrency requirements, the format of the model&#8217;s data types, and any performance requirements such as Time to First Token (TTFT) or throughput.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_estimating_model_size"><a class="anchor" href="#_estimating_model_size"></a>Estimating Model Size</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One of the first calculations needed to help understand if a model will run on a particular GPU is creating an estimate of the model size.</p>
</div>
<div class="paragraph">
<p>The size of the model loaded into vRAM can be estimated using the following formula:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/model-sizing-formula.png" alt="Model Sizing Formula">
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Symbol</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Description</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">M</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GPU memory</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">P</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of parameters in the model</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">4b</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4 bytes, expressing the bytes used for each parameter</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">32</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">There are 32 bits in 4 bytes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Q</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The amount of bits that should be used for loading the model. - 16 bits, 8 bits or 4 bits.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Represents a 20% overhead of loading additional things in GPU memory.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example, for <a href="https://huggingface.co/ibm-granite/granite-3.3-8b-instruct/tree/main">Granite 3.3 8B Instruct</a> is 8 billion parameters and is an fp16 model.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">(((8*10^9*4)/(32/16))*1.2) / 1024^3 = 17.9 Gb</code></pre>
</div>
</div>
<div class="paragraph">
<p>For some additional details on this calculation, you can read more about it <a href="https://training.continuumlabs.ai/infrastructure/data-and-memory/calculating-gpu-memory-for-serving-llms">here</a>.</p>
</div>
<div class="sect2">
<h3 id="_quantization"><a class="anchor" href="#_quantization"></a>Quantization</h3>
<div class="paragraph">
<p>Some LLMs will use quantization to make them smaller and more performant.  Most models will default to an fp16 data type which uses 16 bits, but fp8 is a common quantization option that use 8 bit data types.  Int4 is another common data type that is gaining popularity which uses only 4 bits.  Additionally, some advanced techniques can be uses such as the w4a16 which primarily use an int4 data type but also leverage some additional bits to increase the performance compared to traditional int4 models.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_estimating_kv_cache"><a class="anchor" href="#_estimating_kv_cache"></a>Estimating KV Cache</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Beyond the model itself, vLLM also requires vRAM for the KV Cache.  The KV Cache requires a specific amount of vRAM per token that can be different from model to model.  An estimate for the vRAM per token can be calculated based on the architecture of the model with details from the config.json file on HuggingFace.</p>
</div>
<div class="paragraph">
<p>This class will not dig into details of calculating the vRAM per token, but you can read more <a href="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">here</a> to better understand how to calculate that value.</p>
</div>
<div class="paragraph">
<p>In addition to the vRAM per token, you will need to understand the context length requirements for the use case.  By default vLLM will use the max context length for the model, but if you are running the model on a smaller GPU, you may need to limit the KV Cache to a smaller context length.</p>
</div>
<div class="paragraph">
<p>Granite 3.3 8B Instruct requires 0.15625 Mb per token and the models max context length is 131,072, giving us a requirement of 20 Gb of vRAM for the KV Cache.</p>
</div>
<div class="paragraph">
<p>Along side our model requirements of about 17.9 Gb, that means we need about 38 Gb to run the model and support the max context length.</p>
</div>
<div class="sect2">
<h3 id="_exercise_kv_cache_estimation"><a class="anchor" href="#_exercise_kv_cache_estimation"></a>Exercise: KV Cache Estimation</h3>
<div class="paragraph">
<p>If we had an A10G or an L4 GPU with 24 Gb of vRAM, estimate how many tokens you could configure the KV Cache that would still fit on that device with Granite 3.3 8B Instruct.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sizing_spreadsheet"><a class="anchor" href="#_sizing_spreadsheet"></a>Sizing Spreadsheet</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat Services has created a spreadsheet to help do sizing estimates for LLMs that you can find here:</p>
</div>
<div class="paragraph">
<p><a href="https://red.ht/llm-sizing" class="bare">https://red.ht/llm-sizing</a></p>
</div>
<div class="paragraph">
<p>Using the "Model Sizing" tab, you can select several different models to perform a sizing calculation.  The spreadsheet will provide recommendations based on the model you have selected and the models max context length.</p>
</div>
<div class="paragraph">
<p>You can update the context length requirements to override the default.</p>
</div>
<div class="paragraph">
<p>Additionally, vLLM allows users to use a quantized KV Cache to increase the total number of tokens supported with the same vRAM.</p>
</div>
<div class="paragraph">
<p>In addition to the model sizing, the "Subs &amp; Cost Modeling" tab will help to recommend instance types needed to run your selected model in different environments.  The "Subs &amp; Cost Modeling" tab can help to provide a Total Cost of Ownership of running the models on OpenShift including both cloud computing costs, OpenShift subs, and OpenShift AI subs.</p>
</div>
<div class="paragraph">
<p>Please keep in mind that all costs are subject to change and the cost is simply an estimate and should not be used in a quote to a customer.</p>
</div>
<div class="sect2">
<h3 id="_exercise_model_sizing_quantization_comparison"><a class="anchor" href="#_exercise_model_sizing_quantization_comparison"></a>Exercise: Model Sizing Quantization Comparison</h3>
<div class="paragraph">
<p>Make a copy of the spreadsheet and perform a sizing for an un-quantized model and a RedHatAI quantized version of the same model, such as Llama 70B.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="kserve-gitops.html">GitOps with KServe</a></span>
  <span class="next"><a href="multi-node-multi-gpu.html">Deploying a Model with vLLM on a multiple node with multiple GPUs</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
