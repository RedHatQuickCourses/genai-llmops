<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Deployment across Red Hat AI :: LLM Operations Optimization and Inference</title>
    <link rel="next" href="chapter1/intro.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="chapter1/intro.html">Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#intro.adoc">intro.adoc</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="chapter3/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="chapter3/index.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter3/section1.html">Module 1: Your First Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter3/section2.html">Check your work</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter3/section3.html">Module 2: Automating Quantization with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="chapter3/section4.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="index.html">LLM Deployment across Red Hat AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">LLM Deployment across Red Hat AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to our LLM Optimization and Inferencing hands-on workshop, where you will gain technical experience serving models with vLLM and optimizing performance and accuracy in a number of ways.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_overview"><a class="anchor" href="#_module_overview"></a>Module Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This workshop provides hands-on experience with enterprise vLLM deployment, benchmarking, and optimization techniques. You&#8217;ll learn to deploy models efficiently, evaluate performance, and implement advanced optimization techniques to reduce costs while maintaining quality.</p>
</div>
<div class="sect2">
<h3 id="_module_1_llm_deployment"><a class="anchor" href="#_module_1_llm_deployment"></a>üöÄ Module 1: LLM Deployment</h3>
<div class="paragraph">
<p><strong>Deploy RH Inference Server across multiple platforms</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>1.1 RHEL Deployment</strong>: Set up inference server on Red Hat Enterprise Linux with GPU support, container toolkit configuration, and model serving</p>
</li>
<li>
<p><strong>1.2 OpenShift Deployment</strong>: Deploy using Helm charts and container orchestration for scalable inference</p>
</li>
<li>
<p><strong>1.3 OpenShift AI Deployment</strong>: Leverage Red Hat OpenShift AI platform for managed LLM serving with enterprise features</p>
</li>
<li>
<p><strong>1.4 Platform Comparison</strong>: Understand deployment trade-offs and choose the right platform for your use case</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Key Skills</strong>: Infrastructure setup, containerization, GPU configuration, cloud-native deployment</p>
</div>
</div>
<div class="sect2">
<h3 id="_module_2_performance_accuracy_evaluation"><a class="anchor" href="#_module_2_performance_accuracy_evaluation"></a>üìä Module 2: Performance &amp; Accuracy Evaluation</h3>
<div class="paragraph">
<p><strong>Measure and benchmark LLM systems for production readiness</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>2.1 Performance Evaluation</strong>: Use GuideLLM to measure latency, throughput, and resource utilization under realistic workloads</p>
</li>
<li>
<p><strong>2.2 Accuracy Assessment</strong>: Evaluate model quality, response relevance, and task-specific performance metrics</p>
</li>
<li>
<p><strong>2.3 Evaluation Best Practices</strong>: Establish benchmarking workflows and continuous performance monitoring</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Key Skills</strong>: Performance testing, quality assessment, benchmarking methodologies, production readiness validation</p>
</div>
</div>
<div class="sect2">
<h3 id="_module_3_vllm_optimization"><a class="anchor" href="#_module_3_vllm_optimization"></a>‚ö° Module 3: vLLM Optimization</h3>
<div class="paragraph">
<p><strong>Maximize inference performance through tuning and configuration</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>3.1 Performance Tuning</strong>: Hands-on optimization of granite-3.3-8b-instruct for minimal latency in chat applications</p>
</li>
<li>
<p><strong>3.2 Configuration Strategies</strong>: Master vLLM parameters, memory management, and batching for optimal performance</p>
</li>
<li>
<p><strong>3.3 Scaling Techniques</strong>: Implement strategies for high-throughput serving and resource efficiency</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Key Skills</strong>: Performance optimization, parameter tuning, inference scaling, latency reduction</p>
</div>
</div>
<div class="sect2">
<h3 id="_module_4_model_quantization"><a class="anchor" href="#_module_4_model_quantization"></a>üî¨ Module 4: Model Quantization</h3>
<div class="paragraph">
<p><strong>Reduce model size and memory requirements without sacrificing quality</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>4.1 Quantization Fundamentals</strong>: Understand W4A16, W8A8 schemes and their impact on performance and accuracy</p>
</li>
<li>
<p><strong>4.2 Implementation Labs</strong>: Hands-on quantization using LLM Compressor with SmoothQuant and GPTQ techniques</p>
</li>
<li>
<p><strong>4.3 Production Pipelines</strong>: Build automated quantization workflows using OpenShift AI and evaluate results</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Key Skills</strong>: Model compression, quantization techniques, memory optimization, automated ML pipelines</p>
</div>
</div>
<div class="sect2">
<h3 id="_reference_materials"><a class="anchor" href="#_reference_materials"></a>üìö Reference Materials</h3>
<div class="paragraph">
<p><strong>Business and technical guides for real-world application</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Enterprise Qualification Guide</strong>: Framework for identifying and qualifying LLM optimization opportunities with enterprise clients</p>
</li>
<li>
<p><strong>Technical Deep Dives</strong>: Comprehensive technical documentation on quantization methods and optimization strategies</p>
</li>
<li>
<p><strong>Model Comparison Examples</strong>: Pre-compressed model performance comparisons and selection criteria</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_learning_outcomes"><a class="anchor" href="#_learning_outcomes"></a>üéØ Learning Outcomes</h3>
<div class="paragraph">
<p>By the end of this workshop, you will be able to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy production-ready LLM inference servers across multiple platforms</p>
</li>
<li>
<p>Evaluate and benchmark LLM systems for performance and accuracy</p>
</li>
<li>
<p>Optimize vLLM configurations for specific use cases and constraints</p>
</li>
<li>
<p>Implement quantization techniques to reduce costs by 50-75%</p>
</li>
<li>
<p>Build automated optimization pipelines for enterprise deployment</p>
</li>
<li>
<p>Qualify and position LLM optimization opportunities with technical confidence</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_workshop_format"><a class="anchor" href="#_workshop_format"></a>‚è±Ô∏è Workshop Format</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Duration</strong>: Full-day technical workshop</p>
</li>
<li>
<p><strong>Format</strong>: Mix of theory, hands-on labs, and real-world scenarios</p>
</li>
<li>
<p><strong>Prerequisites</strong>: Basic familiarity with containers, Kubernetes, and machine learning concepts</p>
</li>
<li>
<p><strong>Environment</strong>: Access to OpenShift cluster with GPU resources</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="next"><a href="chapter1/intro.html">Home</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
