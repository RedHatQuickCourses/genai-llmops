<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Module 4: Model Quantization :: LLM Operations Optimization and Inference</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="module-4.1-quantization.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/module-3.0-optimization-intro.html">Module 3: vLLM Performance Optimization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/module-3.1-optimization-practice.html">vLLM &amp; Performance Tuning</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/module-3.2-optimization-conclusion.html">Module 3 Conclusion: vLLM Optimization Mastery</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Module 4: Model Quantization</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="module-4.0-quantization-intro.html">Module 4: Model Quantization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-4.1-quantization.html">Weights and Activation Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-4.2-quantization.html">Model Quantization Pipeline</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-4.3-quantization-conclusion.html">Module 4 Conclusion: Model Quantization Mastery</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM Overview</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM and Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">Optimizing with NVIDIA GPU Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">The Red Hat AI Validated Model Repository</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Intro Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">The Granite Model Family</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/guide_llm.html">Performance Benchmarking with GuideLLM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/kserve-gitops.html">GitOps with KServe</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/llm-sizing.html">LLM GPU Requirements</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/multi-node-multi-gpu.html">Deploying a Model with vLLM on a multiple node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/single-node-multi-gpu.html">Deploying a Model with vLLM on a single node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/vllm-configuration.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="index.html">Module 4: Model Quantization</a></li>
    <li><a href="module-4.0-quantization-intro.html">Module 4: Model Quantization</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Module 4: Model Quantization</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Having optimized your vLLM performance, it&#8217;s time to tackle the most impactful optimization technique: model quantization. This module teaches you to compress LLM weights and activations to dramatically reduce memory requirements and inference costs while preserving model quality.</p>
</div>
<div class="paragraph">
<p>Quantization is transformative because it addresses the fundamental challenge of modern LLMs: their massive size. By reducing numerical precision from 16-bit to 8-bit or 4-bit representations, you can achieve 50-75% memory reduction, enabling deployment on smaller hardware and significant cost savings.</p>
</div>
<div class="paragraph">
<p>Why quantization matters:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cost Reduction</strong>: Deploy larger models on less expensive hardware, reducing infrastructure costs by 2-4X</p>
</li>
<li>
<p><strong>Memory Efficiency</strong>: Fit models that previously required multiple GPUs onto single GPU deployments</p>
</li>
<li>
<p><strong>Inference Speed</strong>: Reduced data movement and optimized compute paths can improve throughput</p>
</li>
<li>
<p><strong>Democratization</strong>: Make state-of-the-art models accessible to organizations with limited GPU budgets</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_learning_objectives"><a class="anchor" href="#_learning_objectives"></a>Learning Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By the end of this module, you will be able to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Understand quantization fundamentals: weights, activations, and precision trade-offs</p>
</li>
<li>
<p>Implement W4A16 and W8A8 quantization schemes using LLM Compressor</p>
</li>
<li>
<p>Apply advanced techniques like SmoothQuant and GPTQ for optimal accuracy preservation</p>
</li>
<li>
<p>Build automated quantization pipelines using OpenShift AI and evaluate compressed models</p>
</li>
<li>
<p>Make informed decisions about quantization schemes based on hardware and use case requirements</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_youll_learn"><a class="anchor" href="#_what_youll_learn"></a>What You&#8217;ll Learn</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_quantization_fundamentals"><a class="anchor" href="#_quantization_fundamentals"></a>Quantization Fundamentals</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Precision Formats</strong>: Understanding FP16, INT8, INT4 and their memory/performance implications</p>
</li>
<li>
<p><strong>Weight vs Activation Quantization</strong>: When and how to quantize different model components</p>
</li>
<li>
<p><strong>Quantization Schemes</strong>: W4A16, W8A8, and selecting the right approach for your hardware</p>
</li>
<li>
<p><strong>Quality vs Efficiency Trade-offs</strong>: Balancing compression ratio with model accuracy</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_advanced_quantization_techniques"><a class="anchor" href="#_advanced_quantization_techniques"></a>Advanced Quantization Techniques</h3>
<div class="ulist">
<ul>
<li>
<p><strong>SmoothQuant</strong>: Smoothing activation outliers for better weight/activation quantization</p>
</li>
<li>
<p><strong>GPTQ</strong>: Layer-wise quantization optimization for minimal accuracy loss</p>
</li>
<li>
<p><strong>Calibration Datasets</strong>: Selecting representative data for optimal quantization parameters</p>
</li>
<li>
<p><strong>Hardware Considerations</strong>: Matching quantization schemes to GPU capabilities (Ampere vs Hopper)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_production_implementation"><a class="anchor" href="#_production_implementation"></a>Production Implementation</h3>
<div class="ulist">
<ul>
<li>
<p><strong>LLM Compressor Workflows</strong>: Using the industry-leading quantization toolkit</p>
</li>
<li>
<p><strong>Pipeline Automation</strong>: Building repeatable quantization workflows in OpenShift AI</p>
</li>
<li>
<p><strong>Quality Evaluation</strong>: Measuring accuracy impact and performance improvements</p>
</li>
<li>
<p><strong>Deployment Integration</strong>: Serving quantized models with vLLM for production workloads</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_structure"><a class="anchor" href="#_module_structure"></a>Module Structure</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_4_1_quantization_fundamentals"><a class="anchor" href="#_4_1_quantization_fundamentals"></a>4.1 Quantization Fundamentals</h3>
<div class="paragraph">
<p>Deep dive into quantization theory, precision formats, and decision frameworks for selecting optimal quantization schemes based on your specific hardware and accuracy requirements.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_2_hands_on_quantization_lab"><a class="anchor" href="#_4_2_hands_on_quantization_lab"></a>4.2 Hands-on Quantization Lab</h3>
<div class="paragraph">
<p>Practical implementation using LLM Compressor to quantize models with W4A16 techniques, including SmoothQuant and GPTQ optimization for maximum quality preservation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_3_production_quantization_pipelines"><a class="anchor" href="#_4_3_production_quantization_pipelines"></a>4.3 Production Quantization Pipelines</h3>
<div class="paragraph">
<p>Build automated, repeatable quantization workflows using OpenShift AI, evaluate results, and integrate quantized models into production vLLM deployments.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting this module, ensure you have:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Completed Modules 1-3 (Deployment, Evaluation, Optimization)</p>
</li>
<li>
<p>Understanding of vLLM serving and performance concepts</p>
</li>
<li>
<p>Access to OpenShift AI environment with GPU resources</p>
</li>
<li>
<p>Familiarity with model evaluation and benchmarking from previous modules</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_real_world_impact"><a class="anchor" href="#_real_world_impact"></a>Real-World Impact</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Consider these quantization results from enterprise deployments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Memory Reduction</strong>: 70B parameter models reduced from 140GB to 35GB (W4A16)</p>
</li>
<li>
<p><strong>Cost Savings</strong>: 400B parameter model deployment cost reduced by 60% through quantization</p>
</li>
<li>
<p><strong>Hardware Accessibility</strong>: Models requiring 8x A100 GPUs compressed to run on 2x A100 GPUs</p>
</li>
<li>
<p><strong>Maintained Quality</strong>: &lt;2% accuracy degradation with proper quantization techniques</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_success_metrics"><a class="anchor" href="#_success_metrics"></a>Success Metrics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By module completion, you should achieve:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Successful Model Compression</strong>: Reduce model memory footprint by 50-75%</p>
</li>
<li>
<p><strong>Quality Preservation</strong>: Maintain &gt;95% of original model accuracy</p>
</li>
<li>
<p><strong>Production Pipeline</strong>: Automated quantization workflow ready for enterprise deployment</p>
</li>
<li>
<p><strong>Cost Analysis</strong>: Clear understanding of infrastructure savings and deployment options</p>
</li>
<li>
<p><strong>Technical Confidence</strong>: Ability to recommend and implement quantization strategies for different use cases</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_whats_next"><a class="anchor" href="#_whats_next"></a>What&#8217;s Next</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This module bridges the gap between research and production, giving you the tools to deploy enterprise-grade compressed models that maintain quality while dramatically reducing costs.</p>
</div>
<div class="paragraph">
<p>Ready to unlock the full potential of LLM quantization? Let&#8217;s begin!</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Module 4: Model Quantization</a></span>
  <span class="next"><a href="module-4.1-quantization.html">Weights and Activation Quantization (W4A16)</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
