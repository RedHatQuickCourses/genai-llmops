<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Module 2: Hands-On Parameter Tuning :: LLM Operations Optimization and Inference</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Evaluating System Performance with GuideLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section1.html">Introduction to Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section2.html">Running Your First Benchmark</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section3.html">Interpreting Benchmark Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section4.html">Advanced Benchmarking Scenarios</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section1.html">Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section2.html">Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section3.html">Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section4.html">Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter2/section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#intro.adoc">intro.adoc</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/mission.html">Your Place in the Adventure</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Model Quantization with LLM Compressor</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section1.html">Module 1: Your First Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section2.html">Check your work</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section3.html">Module 2: Automating Quantization with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter3/section4.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="section2.html">Module 2: Hands-On Parameter Tuning</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Module 2: Hands-On Parameter Tuning</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In this module, we will systematically adjust key vLLM parameters and re-run our benchmark to observe the impact on performance. This iterative process of "tune, measure, repeat" is the core of performance optimization.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_1_tuning_max_model_len"><a class="anchor" href="#_2_1_tuning_max_model_len"></a>2.1 Tuning <code>--max-model-len</code></h3>
<div class="paragraph">
<p>The <code>--max-model-len</code> argument sets the maximum tokens (prompt + response) the server can handle. Setting it too large reserves unnecessary GPU memory for the KV cache, while setting it too small truncates requests.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Let&#8217;s observe the effect of increasing this value. Open your <code>values.yaml</code> file and change the args to increase the length to 4096.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">args:
  - "--disable-log-requests"
  - "--max-num-seqs=32"
  - "--max-model-len=4096"
  - "--max-num-batched-tokens=4096"</code></pre>
</div>
</div>
</li>
<li>
<p>Redeploy the model with <code>helm uninstall</code> and <code>helm install</code>.</p>
</li>
<li>
<p>Once deployed, check the pod logs. You will see that the <code>Available KV cache memory</code> has decreased because more memory is reserved for the larger potential sequence length. This highlights the trade-off: supporting longer sequences leaves less memory for handling concurrent batches. For our chat use case (&lt;2048 tokens), the initial value was more efficient.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_2_2_tuning_max_num_seqs"><a class="anchor" href="#_2_2_tuning_max_num_seqs"></a>2.2 Tuning <code>--max-num-seqs</code></h3>
<div class="paragraph">
<p>The <code>--max-num-seqs</code> parameter defines the maximum number of requests that can be processed in a single batch. If this is set too low, the GPU will be under-utilized and requests will queue up, dramatically increasing latency.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Let&#8217;s simulate a misconfiguration. Revert <code>max-model-len</code> to <code>2048</code> in your <code>values.yaml</code>, but set <code>--max-num-seqs=1</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">args:
  - "--disable-log-requests"
  - "--max-num-seqs=1"
  - "--max-model-len=2048"</code></pre>
</div>
</div>
</li>
<li>
<p>Redeploy the model.</p>
</li>
<li>
<p>Now, edit your <code>guidellm-pipelinerun.yaml</code> to test with just 4 concurrent users by setting <code>rate</code> to <code>4.0</code>.</p>
</li>
<li>
<p>Run the pipeline (<code>oc create -f &#8230;&#8203;</code>).</p>
</li>
<li>
<p>After it finishes, download the new benchmark result in your notebook. Compare the median TTFT for 4 concurrent users from this run to your original <code>benchmark_1.txt</code> file. You will see a massive increase in latency (&gt;6x slower!).</p>
<div class="imageblock unresolved">
<div class="content">
<img src="benchmark-rate4seq1.png" alt="Benchmark with max-num-seqs=1">
</div>
</div>
</li>
<li>
<p>If you check the model&#8217;s pod logs during the run, you&#8217;ll see a clear indicator of the problem: <code>Pending: 31 reqs</code>. The engine is being throttled, processing requests one by one while others wait in a queue. This demonstrates how critical it is to set <code>max-num-seqs</code> appropriately for your expected concurrent load.</p>
</li>
</ol>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
