<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Evaluating Model Accuracy with Trusty AI lm-eval-harness service :: LLM Operations Optimization and Inference</title>
    <link rel="prev" href="module-2.1-eval-performance.html">
    <link rel="next" href="module-2.3-eval-conclusion.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">LLM Operations Optimization and Inference</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-llmops" data-version="2.221">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">LLM Operations Optimization and Inference</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">LLM Deployment across Red Hat AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Module 1: LLM Deployment Strategies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.0-deploy-intro.html">Module 1: LLM Deployment Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.1-deploy-RHEL.html">Deploying RH Inference Server on RHEL</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.2-deploy-ocp.html">Deploying RH Inference Server on OpenShift</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.3-deploy-rhoai.html">Deploy vLLM on OpenShift AI with kserve and model car</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/module-1.4-deploy-conclusion.html">Conclusion to Deployment Exercises</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Module 2: Model Evaluation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-2.0-eval-intro.html">Model Evaluation Module</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-2.1-eval-performance.html">Evaluating System Performance with GuideLLM</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="module-2.2-eval-accuracy.html">Evaluating Model Accuracy with Trusty AI lm-eval-harness service</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="module-2.3-eval-conclusion.html">Conclusion: From Evaluation to Impact</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/module-3.0-optimization-intro.html">Module 3: vLLM Performance Optimization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/module-3.1-optimization-practice.html">vLLM &amp; Performance Tuning</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/module-3.2-optimization-conclusion.html">Module 3 Conclusion: vLLM Optimization Mastery</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Module 4: Model Quantization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.0-quantization-intro.html">Module 4: Model Quantization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.1-quantization.html">Weights and Activation Quantization (W4A16)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.2-quantization.html">Model Quantization Pipeline</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/module-4.3-quantization-conclusion.html">Module 4 Conclusion: Model Quantization Mastery</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM Overview</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM and Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">Optimizing with NVIDIA GPU Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">The Red Hat AI Validated Model Repository</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Intro Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">The Granite Model Family</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/guide_llm.html">Performance Benchmarking with GuideLLM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Advanced vLLM</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/kserve-gitops.html">GitOps with KServe</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/llm-sizing.html">LLM GPU Requirements</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/multi-node-multi-gpu.html">Deploying a Model with vLLM on a multiple node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/single-node-multi-gpu.html">Deploying a Model with vLLM on a single node with multiple GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/vllm-configuration.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">LLM Operations Optimization and Inference</span>
    <span class="version">2.221</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">LLM Operations Optimization and Inference</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.221</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">LLM Operations Optimization and Inference</a></li>
    <li><a href="index.html">Module 2: Model Evaluation</a></li>
    <li><a href="module-2.2-eval-accuracy.html">Evaluating Model Accuracy with Trusty AI lm-eval-harness service</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Evaluating Model Accuracy with Trusty AI lm-eval-harness service</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>While performance metrics like latency and throughput are critical for deploying efficient GenAI systems, <strong>task-level accuracy and reasoning quality</strong> are equally essential for selecting or fine-tuning a model. In this activity, we use the popular lm-eval-harness framework to evaluate how well a language model performs across established benchmarks, focusing on reasoning and subject matter understanding.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_trusty_ai"><a class="anchor" href="#_what_is_trusty_ai"></a>What is Trusty AI?</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://trustyai.org/docs/main/main" target="_blank" rel="noopener">TrustyAI</a> is an open-source AI explainability and trustworthiness platform designed to help developers and data scientists understand and monitor their machine learning models. It provides tools to analyze predictions, identify biases, and ensure that AI systems are fair, transparent, and reliable. As part of its comprehensive toolkit, TrustyAI integrates the popular lm-eval harness to specifically benchmark and evaluate the performance of large language models against standardized tests, allowing users not only to understand why a model makes a decision but also to quantitatively measure its accuracy and capabilities. This combination of explainability and performance evaluation enables organizations to build more responsible, ethical, and robust AI applications.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_lm_eval_harness"><a class="anchor" href="#_what_is_lm_eval_harness"></a>What is lm-eval-harness?</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>lm-eval-harness</strong> is a community-maintained benchmarking toolkit from <strong>EleutherAI</strong>. It enables consistent, reproducible evaluation of large language models (LLMs) across dozens of academic and real-world benchmarks, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>MMLU (Massive Multitask Language Understanding)</p>
</li>
<li>
<p>HellaSwag, ARC, and Winogrande</p>
</li>
<li>
<p>Question answering, common sense reasoning, reading comprehension, and more</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The framework supports both open-source models and OpenAI-compatible endpoints, and can be customized with additional tasks, prompt templates, and evaluation metrics.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mmlu_pro"><a class="anchor" href="#_mmlu_pro"></a>MMLU-Pro</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Today we will be running the mmlu_pro evaluation.</p>
</div>
<div class="paragraph">
<p><strong>MMLU-Pro</strong> is a reasoning-focused, multiple-choice benchmark derived from the original <a href="https://huggingface.co/datasets/cais/mmlu" target="_blank" rel="noopener">MMLU dataset</a>. MMLU-Pro extends the original MMLU benchmark by introducing 10-option multiple-choice questions across diverse academic disciplines. It&#8217;s designed to test a model&#8217;s <strong>reasoning, factual recall, and elimination skills</strong>—key for enterprise AI.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_todays_activity"><a class="anchor" href="#_todays_activity"></a>Today&#8217;s Activity</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section of our lab we will:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Set up the Trusty AI operator</p>
</li>
<li>
<p>Create and run the lm-eval job</p>
</li>
<li>
<p>Interpret and understand results</p>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_setup_trusty_ai"><a class="anchor" href="#_setup_trusty_ai"></a>Setup Trusty AI</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Fist, we&#8217;ll change the trustyai managementState from "Removed" to "Managed" in the default DataScienceCluster.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc patch datasciencecluster default-dsc -p '{"spec":{"components":{"trustyai":{"managementState":"Managed"}}}}' --type=merge</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="https://github.com/trustyai-explainability/llama-stack-provider-lmeval/blob/main/demos/00-getting_started_with_lmeval.ipynb" class="bare">https://github.com/trustyai-explainability/llama-stack-provider-lmeval/blob/main/demos/00-getting_started_with_lmeval.ipynb</a>
<a href="https://trustyai.org/docs/main/lm-eval-tutorial#_examples" class="bare">https://trustyai.org/docs/main/lm-eval-tutorial#_examples</a></p>
</div>
</li>
<li>
<p>Configure TrustyAI to allow downloading remote datasets from Huggingface</p>
<div class="paragraph">
<p>By default, TrustyAI prevents evaluation jobs from accessing the internet or running downloaded code.
A typical evaluation job will download two items from Huggingface:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The dataset of the evaluation task, and any dataset processing code</p>
</li>
<li>
<p>The tokenizer of your model</p>
<div class="paragraph">
<p>If you trust the source of your dataset and tokenizer, you can override TrustyAI&#8217;s default setting.
In our case, we&#8217;ll be downloading:</p>
</div>
</li>
<li>
<p>[allenai/ai2_arc](<a href="https://huggingface.co/datasets/allenai/ai2_arc" class="bare">https://huggingface.co/datasets/allenai/ai2_arc</a>)<br></p>
</li>
<li>
<p>[Phi-3-mini-4k-instruct&#8217;s tokenizer](<a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" class="bare">https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</a>)</p>
<div class="paragraph">
<p>To download those two resources, run:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc patch configmap trustyai-service-operator-config -n redhat-ods-applications  \
--type merge -p '{"metadata": {"annotations": {"opendatahub.io/managed": "false"}}}'
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'
oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Wait for your <code>trustyai-service-operator-controller-manager</code> pod in the <code>redhat-ods-applications</code> namespace
to restart, and then TrustyAI should be ready to go.</p>
</div>
</li>
<li>
<p>Ensure Granite model is configured</p>
<div class="paragraph">
<p>Add the external endpoint to the <strong>workshop_code/evals/trusty/arc_easy.yaml</strong> file in the following section:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">- name: base_url
      value: https://&lt;YOUR_EXTERNAL_INFERENCE_ENDPOINT&gt;/v1/completions # the location of your model's /chat/completions or /completions endpoint</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_create_and_run_the_lm_eval_job"><a class="anchor" href="#_create_and_run_the_lm_eval_job"></a>Create and run the lm-eval job</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run the evaluation</p>
<div class="paragraph">
<p>To start an evaluation, apply an <code>LMEvalJob</code> custom resource as defined in the following file:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f workshop_code/evals/trusty/arc_easy.yaml -n vllm</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check out the arc_easy.yaml file to learn more about the <code>LMEvalJob</code> specification.</p>
</div>
<div class="paragraph">
<p>If everything has worked, you should see a pod called <code>arc-easy-eval-job</code> running in your namespace.
You can watch the progress of your evaluation job by running:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">watch oc logs -f arc-easy-eval-job -n vllm</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see progression in percentage points.</p>
</div>
<div class="paragraph">
<p>or alternatively the logs of the model pod:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc logs -f granite-8b-predictor-&lt;exact-pod-name&gt; -n vllm</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see the exact questions getting passed to the model endpoint.</p>
</div>
<div class="paragraph">
<p>This evaluation run will take approximately 10 minutes.</p>
</div>
</li>
<li>
<p>While You Wait: What is lm-eval-harness?</p>
<div class="ulist">
<ul>
<li>
<p>Check out this <a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb" target="_blank" rel="noopener">overview notebook</a> to explore extensibility and task definitions.</p>
</li>
<li>
<p>View real Red Hat <a href="https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437" target="_blank" rel="noopener">validated model results</a> to understand benchmark outcomes in production contexts and to see how your favorite models rank.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_interpret_and_understand_results"><a class="anchor" href="#_interpret_and_understand_results"></a>Interpret and understand results</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Interpreting MMLU-Pro Results</p>
<div class="paragraph">
<p><strong>Accuracy</strong>: The primary metric is multiple-choice accuracy, indicating how often the model selects the correct answer from 10 options.</p>
</div>
<div class="paragraph">
<p>~10% = random guessing baseline</p>
</div>
<div class="paragraph">
<p>~30-50% = typical for smaller or untuned models</p>
</div>
<div class="paragraph">
<p>~60-70%+ = high reasoning capability or fine-tuned performance</p>
</div>
<div class="paragraph">
<p><strong>Per-subject Scores</strong>: Breakdowns by subject (e.g., philosophy, law, computer science) help identify a model&#8217;s strengths and weaknesses in specific domains.</p>
</div>
<div class="paragraph">
<p><strong>Implications</strong>: Higher MMLU-Pro accuracy generally correlates with better real-world task generalization, especially for tasks involving structured inputs, knowledge retrieval, and logic.</p>
</div>
</li>
<li>
<p>Check out the results</p>
<div class="paragraph">
<p>After the evaluation finishes (it took about 8.5 minutes on my cluster), you can take a look at the results. These are stored in the <code>status.results</code> field of the LMEvalJob resource:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get LMEvalJob arc-easy-eval-job -n vllm -o jsonpath='{.status.results}' | jq '.results'</code></pre>
</div>
</div>
<div class="paragraph">
<p>returns:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.8186026936026936,
    "acc_stderr,none": 0.007907153952801706,
    "acc_norm,none": 0.7836700336700336,
    "acc_norm_stderr,none": 0.00844876352205705
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Explanation of results</strong></p>
</div>
<div class="paragraph">
<p><strong>acc,none</strong>: This stands for accuracy. The value 0.8186 means the model answered approximately 81.86% of the questions correctly based on its raw output.</p>
</div>
<div class="paragraph">
<p><strong>acc_stderr,none</strong>: This is the standard error of the accuracy. The value 0.0079 represents the margin of error for the accuracy score. It indicates how much the result might vary if the test were run again. A smaller number means the result is more statistically reliable.</p>
</div>
<div class="paragraph">
<p><strong>acc_norm,none</strong>: This is the normalized accuracy. The value 0.7836 means that after cleaning up the model&#8217;s answers (e.g., removing extra spaces, punctuation, or standardizing capitalization), it answered about 78.37% of the questions correctly. This score is often considered a more realistic measure of performance.</p>
</div>
<div class="paragraph">
<p><strong>acc_norm_stderr,none</strong>: This is the standard error for the normalized accuracy, indicating the margin of error for that specific score.</p>
</div>
<div class="paragraph">
<p>Now you&#8217;re free to play around with evaluations! You can see the full list of evaluation supported by
lm-evaluation-harness <a href="https://github.com/red-hat-data-services/lm-evaluation-harness/blob/main/lm_eval/tasks/README.md" target="_blank" rel="noopener">here</a></p>
</div>
<div class="paragraph">
<p><strong>More information</strong></p>
</div>
<div class="paragraph">
<p>[TrustyAI Notes Repo](<a href="https://github.com/trustyai-explainability/reference/tree/main" class="bare">https://github.com/trustyai-explainability/reference/tree/main</a>)</p>
</div>
<div class="paragraph">
<p>[TrustyAI Github](<a href="https://github.com/trustyai-explainability" class="bare">https://github.com/trustyai-explainability</a>)</p>
</div>
</li>
<li>
<p>Try MMLU industry-focused test</p>
<div class="paragraph">
<p>In some cases, you may want to check that a model has retained accuracy around a standard, specific dataset topic.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s try the mmlu_jurisprudence dataset to test the model&#8217;s knowledge on law. Update the <strong>base_url</strong> to your external inference endpoint.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f workshop_code/evals/trusty/mmlu_jurisprudence.yaml -n vllm</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will only take a minute or so to process.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get LMEvalJob mmlu-jurisprudence-eval-job -n vllm -o template --template '{{.status.results}}' | jq  .results</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary"><a class="anchor" href="#_summary"></a>Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>What We Did:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Set up TrustyAI operator - Enabled model evaluation framework in OpenShift AI</p>
</li>
<li>
<p>Configured internet access - Allowed downloading of evaluation datasets from HuggingFace</p>
</li>
<li>
<p>Connected to deployed model - Linked evaluation job to the Granite 8B inference service</p>
</li>
<li>
<p>Ran ARC Easy benchmark - Tested model&#8217;s reasoning on grade-school science questions</p>
</li>
<li>
<p>Analyzed results - Achieved 81.8% accuracy, indicating strong reasoning performance</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Key Outcome:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>✅ Successfully evaluated deployed AI model accuracy using industry-standard benchmarks through TrustyAI + lm-eval-harness</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Tools Used:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>TrustyAI: Enterprise evaluation operator</p>
</li>
<li>
<p>lm-eval-harness: Standard benchmarking framework</p>
</li>
<li>
<p>ARC Easy: Science reasoning benchmark</p>
</li>
<li>
<p>Bottom Line: Demonstrated how to measure and validate AI model accuracy in production using automated evaluation pipelines.</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="module-2.1-eval-performance.html">Evaluating System Performance with GuideLLM</a></span>
  <span class="next"><a href="module-2.3-eval-conclusion.html">Conclusion: From Evaluation to Impact</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
